[
  {
    "objectID": "skills.html",
    "href": "skills.html",
    "title": "CompÃ©tences",
    "section": "",
    "text": "ğŸ¯ Mes Domaines d'Expertise\n    Cliquez sur une carte pour explorer les compÃ©tences en dÃ©tail. Survolez les catÃ©gories pour voir le contenu.\n  \n\n  \n    \n    \n    \n      \n        \n      \n      \n      \n        \n          â†\n          ğŸ“š\n          \n            CONNAISSANCES THÃ‰ORIQUES\n            Ã‰conomie, MathÃ©matiques, Statistiques & Ã‰conomÃ©trie\n          \n        \n        \n          \n            \n              \n                â–¶\n                ğŸŒ\n                Langues\n                3\n              \n              \n                ğŸ‡«ğŸ‡· FranÃ§ais (Natif)\n                ğŸ‡¬ğŸ‡§ Anglais (B2)\n              \n            \n            \n              \n                â–¶\n                ğŸ“Š\n                Ã‰conomie\n                5\n              \n              \n                MicroÃ©conomie\n                MacroÃ©conomie\n                Ã‰conomie industrielle\n                ThÃ©orie des jeux\n                Ã‰conomie des plateformes\n              \n            \n            \n              \n                â–¶\n                ğŸ”¢\n                MathÃ©matiques\n                4\n              \n              \n                AlgÃ¨bre linÃ©aire\n                Analyse\n                Optimisation\n                ProbabilitÃ©s\n              \n            \n            \n              \n                â–¶\n                ğŸ“ˆ\n                Statistiques\n                5\n              \n              \n                Tests d'hypothÃ¨ses\n                RÃ©gression\n                SÃ©ries temporelles\n                InfÃ©rence bayÃ©sienne\n                Analyse multivariÃ©e\n              \n            \n            \n              \n                â–¶\n                ğŸ“‰\n                Ã‰conomÃ©trie\n                5\n              \n              \n                Variables instrumentales\n                Diff-in-Diff\n                ARIMA / GARCH\n                Panel Data\n                RDD\n              \n            \n          \n        \n      \n    \n\n    \n    \n      \n  \n\n      \n      \n        \n          â†\n          ğŸ¤–\n          \n            MACHINE LEARNING\n            ModÃ¨les prÃ©dictifs et interprÃ©tation\n          \n        \n        \n          \n            \n              \n                â–¶\n                ğŸ¯\n                Apprentissage SupervisÃ©\n                5\n              \n              \n                RÃ©gression logistique\n                Random Forest\n                XGBoost\n                SVM\n                K-NN\n              \n            \n            \n              \n                â–¶\n                ğŸ”\n                Apprentissage Non SupervisÃ©\n                2\n              \n              \n                K-Means\n                ACM (Analyse des Correspondances Multiples)\n              \n            \n            \n              \n                â–¶\n                âš™ï¸\n                Frameworks & Outils\n                3\n              \n              \n                Scikit-learn\n                Tidymodels (R)\n                SHAP (interprÃ©tation)\n              \n            \n          \n        \n      \n    \n\n    \n    \n      \n  \n\n      \n      \n        \n          â†\n          ğŸ’»\n          \n            TECHNOLOGIES & OUTILS\n            Programmation, DataViz, Bases de donnÃ©es\n          \n        \n        \n          \n            \n              \n                â–¶\n                ğŸ\n                Langages\n                3\n              \n              \n                Python\n                R\n                SQL (MySQL, requÃªtes avancÃ©es)\n              \n            \n            \n              \n                â–¶\n                ğŸ“Š\n                Data Visualization\n                6\n              \n              \n                Plotly / Dash\n                ggplot2 (R)\n                Matplotlib / Seaborn\n                Highcharter (R)\n                Leaflet (cartographie)\n                Power BI\n              \n            \n            \n              \n                â–¶\n                ğŸ—„ï¸\n                Bases de DonnÃ©es & DÃ©ploiement\n                4\n              \n              \n                MySQL (vues, procÃ©dures, fonctions)\n                Git / GitHub\n                Streamlit\n                Shiny (R)\n              \n            \n          \n        \n      \n    \n\n    \n    \n      \n  \n\n      \n      \n        \n          â†\n          ğŸ—£ï¸\n          \n            SOFT SKILLS\n            CompÃ©tences transversales\n          \n        \n        \n          \n            \n              \n                â–¶\n                ğŸ’¡\n                Aptitudes Professionnelles\n                5\n              \n              \n                Analyse Critique\n                Communication\n                Rigueur MÃ©thodologique\n                Problem Solving\n                Gestion de Projet\n              \n            \n          \n        \n      \n    \n\n  \n\n  \n    ğŸ  Accueil\n    /"
  },
  {
    "objectID": "projet1.html",
    "href": "projet1.html",
    "title": "Mon Parcours : Ã‰conomie & Data Science",
    "section": "",
    "text": "Salut ! Je mâ€™appelle Emmanuel Paguiel BOUENDO. Ã‰tudiant en Master Ã‰conomiste dâ€™Entreprise (MÃ‰cEn), je me forme pour devenir le pont entre la data et la stratÃ©gie dâ€™entreprise. Ma double compÃ©tence en Ã©conomie et en Machine Learning me permet de donner du sens stratÃ©gique aux donnÃ©es et de les transformer en recommandations concrÃ¨tes pour les dÃ©cisions.\n\n\n\n\n Master en Data Science / Ã‰conomie\nUniversitÃ© de Tours â€” ğŸ“ Tours, France (2024â€“2025)\nğŸ”— En savoir plus\n Licence en Ã‰conomie de lâ€™entreprise\nUniversitÃ© de Tours â€” ğŸ“ Tours, France (2023â€“2024)\nğŸ”— En savoir plus\n Master Ã‰conomie et Organisation de lâ€™entreprise\nUniversitÃ© Marien Ngouabi â€” ğŸ“ Brazzaville, Congo (2019â€“2021)\nğŸ”— En savoir plus\nLicence en Ã‰conomie de lâ€™entreprise\nUniversitÃ© Marien Ngouabi â€” ğŸ“ Brazzaville, Congo (2015â€“2018)\nLicence professionnelle en Banque, Assurances et Finances\nChambre de Commerce de Brazzaville â€” ğŸ“ Tours, France (2015â€“2016)\n\n\n\nGoogle Data Analytics (Coursera)\nMachine Learning (Stanford / Coursera)\n\n\n\n\n\n\n\nConseiller ClientÃ¨le (Consultant)\nSociÃ©tÃ© GÃ©nÃ©rale Congo (2019â€“2022)\nâ†’ Gestion et dÃ©veloppement dâ€™un portefeuille clients stratÃ©gique, contribuant activement Ã  lâ€™atteinte des objectifs de vente lors de campagnes nationales (voir section SuccÃ¨s ClÃ©s (Better Together, Objectif RÃ©ussi)\nâ†’ Application de techniques de fidÃ©lisation pour amÃ©liorer la rÃ©tention et maximiser la valeur client.\nâ†’ Utilisation dâ€™outils CRM pour analyser les donnÃ©es clients et identifier des opportunitÃ©s de vente croisÃ©e.\nâ†’ MaÃ®trise de la communication, de la nÃ©gociation et de la prÃ©sentation dâ€™informations complexes Ã  divers interlocuteurs.\nâ†’ Collaboration Ã©troite avec les Ã©quipes internes pour assurer une expÃ©rience client fluide et cohÃ©rente.\nGestionnaire de Production IARD (Stage)\nNet Conseils, Brazzaville (2018â€“2019)\nâ†’ Responsable de la saisie et de la gestion administrative des polices dâ€™assurance (contrats, sinistres), garantissant lâ€™intÃ©gritÃ© de lâ€™information.\nâ†’ Suivi des dossiers clients, assurant une communication efficace et une rÃ©solution rapide des demandes.\nâ†’ Participation active aux campagnes de prospection commerciale, contribuant Ã  lâ€™expansion du portefeuille clients.\nAgent de recouvrement (Stage)\nChambre de Commerce de Brazzaville (Marsâ€“Mai 2018)\nâ†’ Coordination des opÃ©rations de recensement, de relance et de collecte des cotisations.\nMiddle Office Risques (Stage)\nLa Congolaise de Banque (2016â€“2017)\nâ†’ Participation Ã  lâ€™analyse des dossiers de crÃ©dit et Ã  lâ€™Ã©laboration de reporting rÃ©guliers sur les risques.\nâ†’ Suivi des indicateurs de performance des portefeuilles de crÃ©dit.\nâ†’ Collaboration avec les Ã©quipes front office pour assurer la conformitÃ© aux politiques de gestion des risques.\nâ†’ Utilisation dâ€™outils analytiques pour identifier les tendances et recommander des actions correctives.\nMiddle Office Clients (Stage)\nFÃ©dÃ©ration des MUCODEC (2016)\nâ†’ Assistance aux opÃ©rations de guichet et accueil client, dÃ©veloppant la capacitÃ© dâ€™adaptation et la gestion des prioritÃ©s.\nâ†’ Suivi organisÃ© des dossiers de crÃ©dits et des opÃ©rations courantes, garantissant la qualitÃ© du service client.\nâ†’ Participation aux activitÃ©s de promotion des services financiers, contribuant Ã  lâ€™augmentation de la clientÃ¨le.\n\n\n\n\n\n\nğŸ–ï¸ DiplÃ´me de succÃ¨s â€“ SociÃ©tÃ© GÃ©nÃ©rale Congo\nğŸ“„ Voir le certificat\nğŸ¥‡ +15% des ouvertures de compte â€“ Better Together\nğŸ“„ Voir le certificat\nğŸš€ 250 ouvertures de compte / trimestre â€“ Objectif RÃ©ussi\nğŸ“„ Voir le certificat\n\n\n\n\n\n\nâ€œLes donnÃ©es racontent des histoires. Mon rÃ´le est de les Ã©couter, les comprendre et les traduire en actions concrÃ¨tes qui crÃ©ent de la valeur.â€\n\nJe crois en une data science : - Humaine et centrÃ©e utilisateur - Ã‰thique et responsable - Accessibles Ã  tous - OrientÃ©e rÃ©sultats concrets\n\n\n\n\nConcevoir des outils IA innovants pour les PME\nPartager mes connaissances via confÃ©rences et articles\nFormer de futurs talents en data science\nContribuer Ã  lâ€™open source Ã  impact social"
  },
  {
    "objectID": "projet1.html#formation",
    "href": "projet1.html#formation",
    "title": "Mon Parcours : Ã‰conomie & Data Science",
    "section": "",
    "text": "Master en Data Science / Ã‰conomie\nUniversitÃ© de Tours â€” ğŸ“ Tours, France (2024â€“2025)\nğŸ”— En savoir plus\n Licence en Ã‰conomie de lâ€™entreprise\nUniversitÃ© de Tours â€” ğŸ“ Tours, France (2023â€“2024)\nğŸ”— En savoir plus\n Master Ã‰conomie et Organisation de lâ€™entreprise\nUniversitÃ© Marien Ngouabi â€” ğŸ“ Brazzaville, Congo (2019â€“2021)\nğŸ”— En savoir plus\nLicence en Ã‰conomie de lâ€™entreprise\nUniversitÃ© Marien Ngouabi â€” ğŸ“ Brazzaville, Congo (2015â€“2018)\nLicence professionnelle en Banque, Assurances et Finances\nChambre de Commerce de Brazzaville â€” ğŸ“ Tours, France (2015â€“2016)\n\n\n\nGoogle Data Analytics (Coursera)\nMachine Learning (Stanford / Coursera)"
  },
  {
    "objectID": "projet1.html#expÃ©rience-professionnelle",
    "href": "projet1.html#expÃ©rience-professionnelle",
    "title": "Mon Parcours : Ã‰conomie & Data Science",
    "section": "",
    "text": "Conseiller ClientÃ¨le (Consultant)\nSociÃ©tÃ© GÃ©nÃ©rale Congo (2019â€“2022)\nâ†’ Gestion et dÃ©veloppement dâ€™un portefeuille clients stratÃ©gique, contribuant activement Ã  lâ€™atteinte des objectifs de vente lors de campagnes nationales (voir section SuccÃ¨s ClÃ©s (Better Together, Objectif RÃ©ussi)\nâ†’ Application de techniques de fidÃ©lisation pour amÃ©liorer la rÃ©tention et maximiser la valeur client.\nâ†’ Utilisation dâ€™outils CRM pour analyser les donnÃ©es clients et identifier des opportunitÃ©s de vente croisÃ©e.\nâ†’ MaÃ®trise de la communication, de la nÃ©gociation et de la prÃ©sentation dâ€™informations complexes Ã  divers interlocuteurs.\nâ†’ Collaboration Ã©troite avec les Ã©quipes internes pour assurer une expÃ©rience client fluide et cohÃ©rente.\nGestionnaire de Production IARD (Stage)\nNet Conseils, Brazzaville (2018â€“2019)\nâ†’ Responsable de la saisie et de la gestion administrative des polices dâ€™assurance (contrats, sinistres), garantissant lâ€™intÃ©gritÃ© de lâ€™information.\nâ†’ Suivi des dossiers clients, assurant une communication efficace et une rÃ©solution rapide des demandes.\nâ†’ Participation active aux campagnes de prospection commerciale, contribuant Ã  lâ€™expansion du portefeuille clients.\nAgent de recouvrement (Stage)\nChambre de Commerce de Brazzaville (Marsâ€“Mai 2018)\nâ†’ Coordination des opÃ©rations de recensement, de relance et de collecte des cotisations.\nMiddle Office Risques (Stage)\nLa Congolaise de Banque (2016â€“2017)\nâ†’ Participation Ã  lâ€™analyse des dossiers de crÃ©dit et Ã  lâ€™Ã©laboration de reporting rÃ©guliers sur les risques.\nâ†’ Suivi des indicateurs de performance des portefeuilles de crÃ©dit.\nâ†’ Collaboration avec les Ã©quipes front office pour assurer la conformitÃ© aux politiques de gestion des risques.\nâ†’ Utilisation dâ€™outils analytiques pour identifier les tendances et recommander des actions correctives.\nMiddle Office Clients (Stage)\nFÃ©dÃ©ration des MUCODEC (2016)\nâ†’ Assistance aux opÃ©rations de guichet et accueil client, dÃ©veloppant la capacitÃ© dâ€™adaptation et la gestion des prioritÃ©s.\nâ†’ Suivi organisÃ© des dossiers de crÃ©dits et des opÃ©rations courantes, garantissant la qualitÃ© du service client.\nâ†’ Participation aux activitÃ©s de promotion des services financiers, contribuant Ã  lâ€™augmentation de la clientÃ¨le."
  },
  {
    "objectID": "projet1.html#rÃ©alisations-marquantes",
    "href": "projet1.html#rÃ©alisations-marquantes",
    "title": "Mon Parcours : Ã‰conomie & Data Science",
    "section": "",
    "text": "ğŸ–ï¸ DiplÃ´me de succÃ¨s â€“ SociÃ©tÃ© GÃ©nÃ©rale Congo\nğŸ“„ Voir le certificat\nğŸ¥‡ +15% des ouvertures de compte â€“ Better Together\nğŸ“„ Voir le certificat\nğŸš€ 250 ouvertures de compte / trimestre â€“ Objectif RÃ©ussi\nğŸ“„ Voir le certificat"
  },
  {
    "objectID": "projet1.html#ma-philosophie",
    "href": "projet1.html#ma-philosophie",
    "title": "Mon Parcours : Ã‰conomie & Data Science",
    "section": "",
    "text": "â€œLes donnÃ©es racontent des histoires. Mon rÃ´le est de les Ã©couter, les comprendre et les traduire en actions concrÃ¨tes qui crÃ©ent de la valeur.â€\n\nJe crois en une data science : - Humaine et centrÃ©e utilisateur - Ã‰thique et responsable - Accessibles Ã  tous - OrientÃ©e rÃ©sultats concrets"
  },
  {
    "objectID": "projet1.html#objectifs",
    "href": "projet1.html#objectifs",
    "title": "Mon Parcours : Ã‰conomie & Data Science",
    "section": "",
    "text": "Concevoir des outils IA innovants pour les PME\nPartager mes connaissances via confÃ©rences et articles\nFormer de futurs talents en data science\nContribuer Ã  lâ€™open source Ã  impact social"
  },
  {
    "objectID": "projects/projet1.html",
    "href": "projects/projet1.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\n\ntitle: â€œAnalyse des Ventes E-commerceâ€\nsubtitle: â€œPrÃ©diction des tendances avec Machine Learningâ€\ndate: â€œ2024-12-01â€\ncategories: [Python, Machine Learning, E-commerce]\nimage: â€œimages/projet1-preview.pngâ€\n\n## Contexte du Projet\nCe projet vise Ã  analyser les donnÃ©es de vente dâ€™un site e-commerce pour prÃ©dire les tendances futures et optimiser les stratÃ©gies marketing.\n## Objectifs\n- Analyser les patterns de vente\n- Identifier les produits les plus performants\n- PrÃ©dire les ventes futures\n- Recommander des actions marketing\n## DonnÃ©es UtilisÃ©es\n- **Source** : Dataset de ventes e-commerce (50k transactions)\n- **PÃ©riode** : Janvier 2023 - DÃ©cembre 2023\n- **Variables** : Date, produit, prix, quantitÃ©, client, rÃ©gion\n## MÃ©thodologie\n### 1. Exploration des DonnÃ©es\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Chargement des donnÃ©es\ndf = pd.read_csv(â€˜ventes_ecommerce.csvâ€™)\n# Exploration initiale\nprint(df.head())\nprint(df.info())\nprint(df.describe())\n\nAnalyse Exploratoire\n\n```python\n\nÃ‰volution des ventes dans le temps\nplt.figure(figsize=(12, 6)) df.groupby(â€˜dateâ€™)[â€˜chiffre_affairesâ€™].sum().plot() plt.title(â€˜Ã‰volution du Chiffre dâ€™Affairesâ€™) plt.show()\n\n\nTop 10 des produits\ntop_products = df.groupby(â€˜produitâ€™)[â€˜quantiteâ€™].sum().nlargest(10) plt.figure(figsize=(10, 6)) top_products.plot(kind=â€˜barâ€™) plt.title(â€˜Top 10 des Produits les Plus Vendusâ€™) plt.show()"
  },
  {
    "objectID": "porto/data_analysis_app.html",
    "href": "porto/data_analysis_app.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\nlibrary(shiny)\nlibrary(DT)\nlibrary(plotly)\nlibrary(dplyr)\nlibrary(vroom)\n\nInterface utilisateur\nui &lt;- fluidPage(\ntitlePanel(â€œAnalyse Dynamique des DonnÃ©esâ€),\nsidebarLayout(\nsidebarPanel( width = 3,\nfileInput(â€œfileâ€, â€œImporter un fichier CSVâ€, accept = c(â€œ.csvâ€)), actionButton(â€œshow_statsâ€, â€œAfficher les statistiquesâ€),\nhr(),\n  h4(\"Analyse KPI\"),\n  selectInput(\"kpi_column\", \"Variable numÃ©rique pour les KPI :\", choices = NULL),\n  hr(),\n  \n  h4(\"DÃ©tection d'Anomalies\"),\n  checkboxInput(\"show_anomalies\", \"Activer la dÃ©tection d'anomalies\", value = FALSE),\n  radioButtons(\"anomaly_method\", \"MÃ©thode de dÃ©tection :\", \n               choices = c(\"Z-score\" = \"zscore\", \"IQR\" = \"iqr\"), \n               selected = \"zscore\")\n),\n\nmainPanel(\n  width = 9,\n  fluidRow(\n    column(4, div(class = \"custom-value-box blue\", \n                  div(class = \"value\", textOutput(\"mean_value\")), \n                  div(class = \"subtitle\", \"Moyenne\"))),\n    column(4, div(class = \"custom-value-box green\", \n                  div(class = \"value\", textOutput(\"median_value\")), \n                  div(class = \"subtitle\", \"MÃ©diane\"))),\n    column(4, div(class = \"custom-value-box orange\", \n                  div(class = \"value\", textOutput(\"total_value\")), \n                  div(class = \"subtitle\", \"Total\")))\n  ),\n  \n  div(class = \"flex-container\",\n      div(class = \"flex-item\",\n          div(class = \"panel panel-info\",\n              div(class = \"panel-heading\", h3(\"Table des DonnÃ©es Brutes\")),\n              div(class = \"panel-body\", DTOutput(\"data_table\"))\n          )\n      ),\n      div(class = \"flex-item\",\n          div(class = \"panel panel-warning\",\n              div(class = \"panel-heading\", h3(\"Statistiques Descriptives\")),\n              div(class = \"panel-body\", verbatimTextOutput(\"stats_output\"))\n          )\n      )\n  ),\n  \n  div(class = \"graph-container\",\n      lapply(1:2, function(i) {\n        div(class = \"flex-item\",\n            div(class = \"panel panel-primary\",\n                div(class = \"panel-heading\", h3(paste(\"Graphique\", i))),\n                div(class = \"panel-body\",\n                    div(class = \"select-flex\",\n                        selectInput(paste0(\"graph_type\", i), \"Type de graphique :\", \n                                    choices = c(\"Scatter\" = \"scatter\", \"Barres\" = \"bar\", \n                                                \"Ligne\" = \"line\", \"Camembert\" = \"pie\", \n                                                \"Histogramme\" = \"histogram\")),\n                        selectInput(paste0(\"x_var\", i), \"Variable X :\", choices = NULL),\n                        selectInput(paste0(\"y_var\", i), \"Variable Y :\", choices = NULL)\n                    ),\n                    plotlyOutput(paste0(\"graph\", i), height = \"400px\")\n                )\n            )\n        )\n      })\n  )\n)\n) )\n\n\nServeur\nserver &lt;- function(input, output, session) { data &lt;- reactiveVal(NULL)\n# Observer pour mettre Ã  jour les choix de colonnes observeEvent(data(), { req(data()) numeric_columns &lt;- names(data())[sapply(data(), is.numeric)] updateSelectInput(session, â€œkpi_columnâ€, choices = numeric_columns)\nfor (i in 1:2) {\n  updateSelectInput(session, paste0(\"x_var\", i), choices = names(data()))\n  updateSelectInput(session, paste0(\"y_var\", i), choices = names(data()))\n}\n})\n# Importer les donnÃ©es observeEvent(input\\(file, {\n    req(input\\)file) tryCatch({ imported_data &lt;- vroom::vroom(input\\(file\\)datapath, show_col_types = FALSE) data(imported_data) showNotification(â€œDonnÃ©es importÃ©es avec succÃ¨s.â€, type = â€œmessageâ€) }, error = function(e) { showNotification(paste(â€œErreur lors de lâ€™importation:â€, e$message), type = â€œerrorâ€) }) })\n# Afficher les statistiques descriptives observeEvent(input\\(show_stats, {\n    output\\)stats_output &lt;- renderPrint({ req(data()) summary(data()) }) })\n# Calculer et afficher les KPI output\\(mean_value &lt;- renderText({\n    req(data(), input\\)kpi_column) column &lt;- data()[[input$kpi_column]] if (is.numeric(column)) { format(round(mean(column, na.rm = TRUE), 2), nsmall = 2) } else { â€œNAâ€ } })\noutput\\(median_value &lt;- renderText({\n    req(data(), input\\)kpi_column) column &lt;- data()[[input$kpi_column]] if (is.numeric(column)) { format(round(median(column, na.rm = TRUE), 2), nsmall = 2) } else { â€œNAâ€ } })\noutput\\(total_value &lt;- renderText({\n    req(data(), input\\)kpi_column) column &lt;- data()[[input$kpi_column]] if (is.numeric(column)) { format(round(sum(column, na.rm = TRUE), 2), nsmall = 2) } else { â€œNAâ€ } })\n# Fonction pour gÃ©nÃ©rer les graphiques render_graph_plotly &lt;- function(graph_type_input, x_var_input, y_var_input) { renderPlotly({ req(data(), input[[graph_type_input]], input[[y_var_input]])\n  graph_data &lt;- data()\n  y_var_name &lt;- input[[y_var_input]]\n  x_var_name &lt;- input[[x_var_input]]\n  \n  if (is.null(y_var_name)) return(NULL)\n  \n  tryCatch({\n    p &lt;- NULL\n    if (input[[graph_type_input]] == \"scatter\") {\n      p &lt;- plot_ly(graph_data, x = ~get(x_var_name), y = ~get(y_var_name), \n                   type = \"scatter\", mode = \"markers\")\n    } else if (input[[graph_type_input]] == \"bar\") {\n      p &lt;- plot_ly(graph_data, x = ~get(x_var_name), y = ~get(y_var_name), \n                   type = \"bar\")\n    } else if (input[[graph_type_input]] == \"line\") {\n      p &lt;- plot_ly(graph_data, x = ~get(x_var_name), y = ~get(y_var_name), \n                   type = \"scatter\", mode = \"lines\")\n    } else if (input[[graph_type_input]] == \"pie\") {\n      p &lt;- plot_ly(graph_data, labels = ~get(x_var_name), values = ~get(y_var_name), \n                   type = \"pie\")\n    } else if (input[[graph_type_input]] == \"histogram\") {\n      p &lt;- plot_ly(graph_data, x = ~get(y_var_name), type = \"histogram\")\n    } else {\n      return(NULL)\n    }\n    \n    # Ajout des anomalies au graphique si activÃ©\n    if (input$show_anomalies && !is.null(input$kpi_column) && \n        input$kpi_column == y_var_name && is.numeric(graph_data[[y_var_name]])) {\n      anomalies &lt;- detect_anomalies(graph_data[[y_var_name]], method = input$anomaly_method)\n      anomaly_points &lt;- graph_data[anomalies, ]\n      \n      if (nrow(anomaly_points) &gt; 0) {\n        p &lt;- p %&gt;%\n          add_trace(data = anomaly_points,\n                    x = ~get(x_var_name), y = ~get(y_var_name),\n                    type = \"scatter\", mode = \"markers\",\n                    marker = list(color = 'red', size = 10, symbol = 'x'),\n                    name = \"Anomalies\")\n      }\n    }\n    \n    p %&gt;% layout(title = paste(input[[graph_type_input]], \"de\", y_var_name))\n  }, error = function(e) {\n    showNotification(paste(\"Erreur dans le graphique:\", e$message), type = \"error\")\n    NULL\n  })\n})\n}\n# Initialiser les graphiques output\\(graph1 &lt;- render_graph_plotly(\"graph_type1\", \"x_var1\", \"y_var1\")\n  output\\)graph2 &lt;- render_graph_plotly(â€œgraph_type2â€, â€œx_var2â€, â€œy_var2â€)\n# Fonction de dÃ©tection dâ€™anomalies detect_anomalies &lt;- function(vec, method = â€œzscoreâ€, threshold = 3) { if (!is.numeric(vec)) return(rep(FALSE, length(vec)))\nif (method == \"zscore\") {\n  z_scores &lt;- abs(scale(vec, center = TRUE, scale = TRUE))\n  return(as.vector(z_scores &gt; threshold))\n} else if (method == \"iqr\") {\n  Q1 &lt;- quantile(vec, 0.25, na.rm = TRUE)\n  Q3 &lt;- quantile(vec, 0.75, na.rm = TRUE)\n  IQR_val &lt;- Q3 - Q1\n  return(vec &lt; (Q1 - 1.5 * IQR_val) | vec &gt; (Q3 + 1.5 * IQR_val))\n} else {\n  return(rep(FALSE, length(vec)))\n}\n}\n# Table avec anomalies output$data_table &lt;- renderDT({ req(data()) dat &lt;- data()\nif (input$show_anomalies && !is.null(input$kpi_column) && \n    is.numeric(dat[[input$kpi_column]])) {\n  anomalies &lt;- detect_anomalies(dat[[input$kpi_column]], method = input$anomaly_method)\n  dat$Anomalie &lt;- ifelse(anomalies, \"âš ï¸\", \"\")\n  \n  datatable(dat, options = list(pageLength = 10, scrollX = TRUE)) %&gt;%\n    formatStyle(\"Anomalie\", target = \"row\",\n                backgroundColor = styleEqual(\"âš ï¸\", \"#ffcccc\"))\n} else {\n  datatable(dat, options = list(pageLength = 10, scrollX = TRUE))\n}\n}) }\nshinyApp(ui, server)"
  },
  {
    "objectID": "files/Optimisation_Production_Complete.html",
    "href": "files/Optimisation_Production_Complete.html",
    "title": "ğŸ“Š Optimisation de Production - Programmation LinÃ©aire",
    "section": "",
    "text": "Projet : Optimisation de la production de canapÃ©s avec contraintes de ressources\nAuteur : Emmanuel Paguiel BOUENDO\nDate : 2024-2025\nContexte : Master Ã‰conomiste de lâ€™Entreprise - UniversitÃ© de Tours"
  },
  {
    "objectID": "files/Optimisation_Production_Complete.html#objectif-du-projet",
    "href": "files/Optimisation_Production_Complete.html#objectif-du-projet",
    "title": "ğŸ“Š Optimisation de Production - Programmation LinÃ©aire",
    "section": "ğŸ¯ Objectif du projet",
    "text": "ğŸ¯ Objectif du projet\nCe notebook prÃ©sente une analyse complÃ¨te dâ€™optimisation de production utilisant la programmation linÃ©aire. Lâ€™entreprise doit maximiser son bÃ©nÃ©fice sous contraintes de temps, matiÃ¨res premiÃ¨res et capacitÃ©s de marchÃ©.\n\nğŸ“Œ CompÃ©tences dÃ©montrÃ©es\n\nModÃ©lisation mathÃ©matique de problÃ¨mes Ã©conomiques\nProgrammation linÃ©aire et optimisation\nAnalyse de sensibilitÃ©\nVisualisation de donnÃ©es\nPython scientifique (NumPy, SciPy, Matplotlib)\n\n\n\nCode\n# Imports des bibliothÃ¨ques nÃ©cessaires\nimport numpy as np\nimport scipy.optimize as so\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Polygon\nimport pandas as pd\nfrom IPython.display import display, Markdown\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Configuration pour de meilleurs graphiques\nplt.style.use('seaborn-v0_8-darkgrid')\nplt.rcParams['figure.figsize'] = (12, 8)\nplt.rcParams['font.size'] = 11\nplt.rcParams['axes.labelsize'] = 12\nplt.rcParams['axes.titlesize'] = 14\nplt.rcParams['legend.fontsize'] = 10\n\nprint(\"âœ… BibliothÃ¨ques importÃ©es avec succÃ¨s\")"
  },
  {
    "objectID": "files/Optimisation_Production_Complete.html#introduction---contexte-du-problÃ¨me",
    "href": "files/Optimisation_Production_Complete.html#introduction---contexte-du-problÃ¨me",
    "title": "ğŸ“Š Optimisation de Production - Programmation LinÃ©aire",
    "section": "ğŸ“‹ Introduction - Contexte du problÃ¨me",
    "text": "ğŸ“‹ Introduction - Contexte du problÃ¨me\nUne entreprise de fabrication de meubles produit des canapÃ©s de diffÃ©rents modÃ¨les. Elle commence par analyser le marchÃ© avec deux modÃ¨les A et B.\n\nğŸ’° DonnÃ©es Ã©conomiques\n\n\n\nModÃ¨le\nBÃ©nÃ©fice unitaire\nTemps de production\nPrix de vente\n\n\n\n\nA\n300 â‚¬\n2 heures\n800 â‚¬\n\n\nB\n200 â‚¬\n1 heure\n500 â‚¬\n\n\n\n\n\nâš™ï¸ Contraintes de production\n\nTemps de production : Lâ€™usine dispose de 200 heures/mois\nCapacitÃ© de marchÃ© :\n\nMaximum 100 canapÃ©s de type A par mois\nMaximum 100 canapÃ©s de type B par mois\nMaximum 150 canapÃ©s au total par mois\n\nContraintes de matiÃ¨res premiÃ¨res (voir partie B)\n\n\n\nğŸ¯ Question centrale\nCombien de canapÃ©s de chaque type lâ€™entreprise doit-elle produire pour maximiser son bÃ©nÃ©fice ?"
  },
  {
    "objectID": "files/Optimisation_Production_Complete.html#partie-1-modÃ©lisation-mathÃ©matique",
    "href": "files/Optimisation_Production_Complete.html#partie-1-modÃ©lisation-mathÃ©matique",
    "title": "ğŸ“Š Optimisation de Production - Programmation LinÃ©aire",
    "section": "ğŸ“ Partie 1 : ModÃ©lisation mathÃ©matique",
    "text": "ğŸ“ Partie 1 : ModÃ©lisation mathÃ©matique\n\nğŸ”¢ Formulation du problÃ¨me de programmation linÃ©aire\nVariables de dÃ©cision : - \\(x_1\\) : nombre de canapÃ©s de modÃ¨le A produits - \\(x_2\\) : nombre de canapÃ©s de modÃ¨le B produits\nFonction objectif (Ã  maximiser) : \\[\\max Z = 300x_1 + 200x_2\\]\nContraintes : \\[\n\\begin{aligned}\n& 0 \\leq x_1 \\leq 100 & \\text{(capacitÃ© marchÃ© A)} \\\\\n& 0 \\leq x_2 \\leq 100 & \\text{(capacitÃ© marchÃ© B)} \\\\\n& 2x_1 + x_2 \\leq 200 & \\text{(temps de production)} \\\\\n& x_1 + x_2 \\leq 150 & \\text{(capacitÃ© totale)}\n\\end{aligned}\n\\]\n\n\nCode\n# DÃ©finition des paramÃ¨tres du problÃ¨me\nclass ProblemeOptimisation:\n    \"\"\"Classe pour encapsuler le problÃ¨me d'optimisation\"\"\"\n    \n    def __init__(self):\n        # Coefficients de la fonction objectif (bÃ©nÃ©fices)\n        self.c = [-300, -200]  # NÃ©gatif car scipy minimise par dÃ©faut\n        \n        # Contraintes d'inÃ©galitÃ© (A_ub @ x &lt;= b_ub)\n        self.A_ub = np.array([\n            [2, 1],   # Temps de production\n            [1, 1]    # CapacitÃ© totale\n        ])\n        self.b_ub = np.array([200, 150])\n        \n        # Bornes des variables\n        self.bounds = [(0, 100), (0, 100)]  # x1 et x2 entre 0 et 100\n        \n    def fonction_objectif(self, x):\n        \"\"\"Calcule le bÃ©nÃ©fice pour un plan de production donnÃ©\"\"\"\n        return 300 * x[0] + 200 * x[1]\n    \n    def afficher_parametres(self):\n        \"\"\"Affiche les paramÃ¨tres du problÃ¨me\"\"\"\n        print(\"ğŸ“Š ParamÃ¨tres du problÃ¨me d'optimisation\")\n        print(\"=\" * 50)\n        print(f\"BÃ©nÃ©fice modÃ¨le A : {-self.c[0]} â‚¬\")\n        print(f\"BÃ©nÃ©fice modÃ¨le B : {-self.c[1]} â‚¬\")\n        print(f\"\\nContrainte temps : 2xâ‚ + xâ‚‚ â‰¤ {self.b_ub[0]}\")\n        print(f\"Contrainte capacitÃ© : xâ‚ + xâ‚‚ â‰¤ {self.b_ub[1]}\")\n        print(f\"\\nBornes : 0 â‰¤ xâ‚ â‰¤ {self.bounds[0][1]}, 0 â‰¤ xâ‚‚ â‰¤ {self.bounds[1][1]}\")\n\n# CrÃ©ation de l'instance du problÃ¨me\nprobleme = ProblemeOptimisation()\nprobleme.afficher_parametres()"
  },
  {
    "objectID": "files/Optimisation_Production_Complete.html#partie-2-rÃ©solution-numÃ©rique",
    "href": "files/Optimisation_Production_Complete.html#partie-2-rÃ©solution-numÃ©rique",
    "title": "ğŸ“Š Optimisation de Production - Programmation LinÃ©aire",
    "section": "ğŸ” Partie 2 : RÃ©solution numÃ©rique",
    "text": "ğŸ” Partie 2 : RÃ©solution numÃ©rique\n\n\nCode\n# RÃ©solution avec la mÃ©thode du simplexe (scipy.optimize.linprog)\ndef resoudre_probleme(probleme):\n    \"\"\"RÃ©sout le problÃ¨me de programmation linÃ©aire\"\"\"\n    \n    result = so.linprog(\n        c=probleme.c,\n        A_ub=probleme.A_ub,\n        b_ub=probleme.b_ub,\n        bounds=probleme.bounds,\n        method='highs'  # Algorithme moderne et efficace\n    )\n    \n    if result.success:\n        x1_opt, x2_opt = result.x\n        benefice_max = -result.fun  # Repassage en maximisation\n        \n        print(\"\\n\" + \"=\" * 60)\n        print(\"ğŸ¯ SOLUTION OPTIMALE TROUVÃ‰E\")\n        print(\"=\" * 60)\n        print(f\"\\nğŸ“¦ Production optimale :\")\n        print(f\"   â€¢ CanapÃ©s modÃ¨le A : {x1_opt:.0f} unitÃ©s\")\n        print(f\"   â€¢ CanapÃ©s modÃ¨le B : {x2_opt:.0f} unitÃ©s\")\n        print(f\"   â€¢ Total canapÃ©s    : {x1_opt + x2_opt:.0f} unitÃ©s\")\n        print(f\"\\nğŸ’° BÃ©nÃ©fice maximal : {benefice_max:,.0f} â‚¬\")\n        print(f\"\\nâ±ï¸  Utilisation du temps : {2*x1_opt + x2_opt:.0f} h / 200 h ({100*(2*x1_opt + x2_opt)/200:.1f}%)\")\n        print(f\"ğŸ“Š Taux d'utilisation capacitÃ© : {100*(x1_opt + x2_opt)/150:.1f}%\")\n        print(\"=\" * 60)\n        \n        return x1_opt, x2_opt, benefice_max\n    else:\n        print(\"âŒ Ã‰chec de l'optimisation\")\n        return None, None, None\n\n# RÃ©solution\nx1_opt, x2_opt, max_benefice = resoudre_probleme(probleme)"
  },
  {
    "objectID": "files/Optimisation_Production_Complete.html#partie-3-visualisation-graphique-de-la-solution",
    "href": "files/Optimisation_Production_Complete.html#partie-3-visualisation-graphique-de-la-solution",
    "title": "ğŸ“Š Optimisation de Production - Programmation LinÃ©aire",
    "section": "ğŸ“ˆ Partie 3 : Visualisation graphique de la solution",
    "text": "ğŸ“ˆ Partie 3 : Visualisation graphique de la solution\n\n\nCode\ndef visualiser_solution(x1_opt, x2_opt, max_benefice):\n    \"\"\"CrÃ©e une visualisation complÃ¨te de la rÃ©gion rÃ©alisable et de la solution\"\"\"\n    \n    fig, ax = plt.subplots(figsize=(14, 10))\n    \n    # GÃ©nÃ©ration des points\n    x1 = np.linspace(0, 110, 1000)\n    \n    # Droites de contraintes\n    x2_temps = 200 - 2*x1        # 2x1 + x2 = 200\n    x2_capacite = 150 - x1       # x1 + x2 = 150\n    x2_marche_B = np.full_like(x1, 100)  # x2 = 100\n    \n    # TracÃ© des contraintes\n    ax.plot(x1, x2_temps, 'b-', linewidth=2.5, label='Contrainte temps (2xâ‚ + xâ‚‚ â‰¤ 200)', alpha=0.8)\n    ax.plot(x1, x2_capacite, 'g-', linewidth=2.5, label='Contrainte capacitÃ© (xâ‚ + xâ‚‚ â‰¤ 150)', alpha=0.8)\n    ax.axvline(x=100, color='orange', linewidth=2.5, label='Contrainte marchÃ© A (xâ‚ â‰¤ 100)', alpha=0.8)\n    ax.axhline(y=100, color='purple', linewidth=2.5, label='Contrainte marchÃ© B (xâ‚‚ â‰¤ 100)', alpha=0.8)\n    \n    # Zone rÃ©alisable (calcul des sommets du polygone)\n    vertices = np.array([\n        [0, 0],\n        [0, 100],\n        [50, 100],\n        [100, 0]\n    ])\n    \n    polygon = Polygon(vertices, alpha=0.3, facecolor='lightblue', \n                     edgecolor='navy', linewidth=2, label='Zone rÃ©alisable')\n    ax.add_patch(polygon)\n    \n    # TracÃ© des courbes iso-profit\n    for benefice in [20000, 30000, max_benefice]:\n        x2_iso = (benefice - 300*x1) / 200\n        style = '--' if benefice != max_benefice else '-'\n        width = 1.5 if benefice != max_benefice else 3\n        alpha = 0.5 if benefice != max_benefice else 1\n        label = f'Iso-profit {benefice:,.0f}â‚¬' if benefice != max_benefice else f'Iso-profit OPTIMAL {benefice:,.0f}â‚¬'\n        ax.plot(x1, x2_iso, style, color='red', linewidth=width, alpha=alpha, label=label)\n    \n    # Point optimal\n    ax.plot(x1_opt, x2_opt, 'r*', markersize=25, markeredgecolor='darkred', \n           markeredgewidth=2, label=f'Solution optimale', zorder=5)\n    \n    # Annotation du point optimal\n    ax.annotate(f'Optimal\\n({x1_opt:.0f}, {x2_opt:.0f})\\n{max_benefice:,.0f}â‚¬',\n               xy=(x1_opt, x2_opt), xytext=(x1_opt+15, x2_opt+15),\n               fontsize=12, fontweight='bold',\n               bbox=dict(boxstyle='round,pad=0.8', facecolor='yellow', alpha=0.8),\n               arrowprops=dict(arrowstyle='-&gt;', connectionstyle='arc3,rad=0.3', \n                             color='red', lw=2))\n    \n    # Configuration des axes\n    ax.set_xlim(-5, 110)\n    ax.set_ylim(-5, 110)\n    ax.set_xlabel('Nombre de canapÃ©s modÃ¨le A (xâ‚)', fontsize=13, fontweight='bold')\n    ax.set_ylabel('Nombre de canapÃ©s modÃ¨le B (xâ‚‚)', fontsize=13, fontweight='bold')\n    ax.set_title('Optimisation de la production de canapÃ©s\\nProgrammation LinÃ©aire - MÃ©thode Graphique', \n                fontsize=15, fontweight='bold', pad=20)\n    \n    # Grille et lÃ©gende\n    ax.grid(True, alpha=0.3, linestyle=':', linewidth=0.8)\n    ax.legend(loc='upper right', fontsize=10, framealpha=0.95, shadow=True)\n    \n    # Ajout d'informations supplÃ©mentaires\n    info_text = f\"Production optimale : {x1_opt:.0f} Ã— A + {x2_opt:.0f} Ã— B = {max_benefice:,.0f}â‚¬\\n\"\n    info_text += f\"Temps utilisÃ© : {2*x1_opt + x2_opt:.0f}h / 200h | CapacitÃ© : {x1_opt+x2_opt:.0f} / 150\"\n    ax.text(0.02, 0.98, info_text, transform=ax.transAxes, fontsize=11,\n           verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n    \n    plt.tight_layout()\n    plt.show()\n\n# GÃ©nÃ©ration du graphique\nvisualiser_solution(x1_opt, x2_opt, max_benefice)"
  },
  {
    "objectID": "files/Optimisation_Production_Complete.html#partie-4-analyse-de-sensibilitÃ©",
    "href": "files/Optimisation_Production_Complete.html#partie-4-analyse-de-sensibilitÃ©",
    "title": "ğŸ“Š Optimisation de Production - Programmation LinÃ©aire",
    "section": "ğŸ”¬ Partie 4 : Analyse de sensibilitÃ©",
    "text": "ğŸ”¬ Partie 4 : Analyse de sensibilitÃ©\nÃ‰tudions comment la solution optimale varie en fonction des paramÃ¨tres clÃ©s.\n\n\nCode\ndef analyse_sensibilite_benefice():\n    \"\"\"Analyse de sensibilitÃ© sur les bÃ©nÃ©fices unitaires\"\"\"\n    \n    # Variation du bÃ©nÃ©fice du modÃ¨le A\n    benefices_A = np.arange(200, 501, 20)\n    resultats = []\n    \n    for b_A in benefices_A:\n        c_temp = [-b_A, -200]\n        result = so.linprog(c=c_temp, A_ub=probleme.A_ub, b_ub=probleme.b_ub, \n                          bounds=probleme.bounds, method='highs')\n        if result.success:\n            resultats.append({\n                'BÃ©nÃ©fice_A': b_A,\n                'x1_optimal': result.x[0],\n                'x2_optimal': result.x[1],\n                'BÃ©nÃ©fice_total': -result.fun\n            })\n    \n    df = pd.DataFrame(resultats)\n    \n    # Visualisation\n    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n    \n    # Graphique 1 : Production optimale\n    axes[0].plot(df['BÃ©nÃ©fice_A'], df['x1_optimal'], 'b-o', linewidth=2, markersize=6, label='ModÃ¨le A')\n    axes[0].plot(df['BÃ©nÃ©fice_A'], df['x2_optimal'], 'g-s', linewidth=2, markersize=6, label='ModÃ¨le B')\n    axes[0].axvline(x=300, color='red', linestyle='--', alpha=0.7, label='Valeur actuelle')\n    axes[0].set_xlabel('BÃ©nÃ©fice unitaire modÃ¨le A (â‚¬)', fontweight='bold')\n    axes[0].set_ylabel('QuantitÃ© optimale', fontweight='bold')\n    axes[0].set_title('Impact du bÃ©nÃ©fice A sur la production optimale', fontweight='bold')\n    axes[0].legend()\n    axes[0].grid(True, alpha=0.3)\n    \n    # Graphique 2 : BÃ©nÃ©fice total\n    axes[1].plot(df['BÃ©nÃ©fice_A'], df['BÃ©nÃ©fice_total'], 'r-o', linewidth=2.5, markersize=6)\n    axes[1].axvline(x=300, color='blue', linestyle='--', alpha=0.7, label='Valeur actuelle')\n    axes[1].fill_between(df['BÃ©nÃ©fice_A'], df['BÃ©nÃ©fice_total'], alpha=0.3, color='red')\n    axes[1].set_xlabel('BÃ©nÃ©fice unitaire modÃ¨le A (â‚¬)', fontweight='bold')\n    axes[1].set_ylabel('BÃ©nÃ©fice total optimal (â‚¬)', fontweight='bold')\n    axes[1].set_title('Impact du bÃ©nÃ©fice A sur le bÃ©nÃ©fice total', fontweight='bold')\n    axes[1].legend()\n    axes[1].grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return df\n\n# ExÃ©cution de l'analyse\ndf_sensibilite = analyse_sensibilite_benefice()\n\n\n\n\nCode\n# Affichage du tableau de sensibilitÃ©\nprint(\"\\nğŸ“Š Tableau de sensibilitÃ© (extrait)\")\nprint(\"=\" * 70)\ndisplay(df_sensibilite[::5].style.format({\n    'BÃ©nÃ©fice_A': '{:.0f} â‚¬',\n    'x1_optimal': '{:.0f}',\n    'x2_optimal': '{:.0f}',\n    'BÃ©nÃ©fice_total': '{:,.0f} â‚¬'\n}).background_gradient(subset=['BÃ©nÃ©fice_total'], cmap='Greens'))"
  },
  {
    "objectID": "files/Optimisation_Production_Complete.html#partie-5-extension---contraintes-de-matiÃ¨res-premiÃ¨res",
    "href": "files/Optimisation_Production_Complete.html#partie-5-extension---contraintes-de-matiÃ¨res-premiÃ¨res",
    "title": "ğŸ“Š Optimisation de Production - Programmation LinÃ©aire",
    "section": "ğŸ“¦ Partie 5 : Extension - Contraintes de matiÃ¨res premiÃ¨res",
    "text": "ğŸ“¦ Partie 5 : Extension - Contraintes de matiÃ¨res premiÃ¨res\nLâ€™entreprise dispose Ã©galement de ressources limitÃ©es en matiÃ¨res premiÃ¨res :\n\n\n\nMatiÃ¨re\nModÃ¨le A\nModÃ¨le B\nStock disponible\n\n\n\n\nBois\n2 unitÃ©s\n1 unitÃ©\n125 unitÃ©s\n\n\nVisserie\n0.2 kg\n0.2 kg\n55 kg\n\n\nAluminium\n1 m\n1 m\n300 m\n\n\nAcier\n2 kg\n2 kg\n100 kg\n\n\nCaoutchouc\n1 mÂ²\n1 mÂ²\n60 mÂ²\n\n\nMousse\n1 mÂ³\n1 mÂ³\n100 mÂ³\n\n\nTissu\n2 m\n2 m\n220 m\n\n\n\n\n\nCode\n# ProblÃ¨me Ã©tendu avec matiÃ¨res premiÃ¨res\nclass ProblemeEtendu:\n    \"\"\"ProblÃ¨me d'optimisation avec contraintes de matiÃ¨res premiÃ¨res\"\"\"\n    \n    def __init__(self):\n        self.c = [-300, -200]\n        \n        # Matrice des contraintes Ã©tendue\n        self.A_ub = np.array([\n            [2, 1],      # Temps\n            [1, 1],      # CapacitÃ© totale\n            [2, 1],      # Bois\n            [0.2, 0.2],  # Visserie\n            [1, 1],      # Aluminium\n            [2, 2],      # Acier\n            [1, 1],      # Caoutchouc\n            [1, 1],      # Mousse\n            [2, 2]       # Tissu\n        ])\n        \n        self.b_ub = np.array([200, 150, 125, 55, 300, 100, 60, 100, 220])\n        self.bounds = [(0, 100), (0, 100)]\n        \n        self.noms_contraintes = [\n            'Temps', 'CapacitÃ©', 'Bois', 'Visserie', \n            'Aluminium', 'Acier', 'Caoutchouc', 'Mousse', 'Tissu'\n        ]\n\nprobleme_etendu = ProblemeEtendu()\n\n# RÃ©solution\nresult_etendu = so.linprog(\n    c=probleme_etendu.c,\n    A_ub=probleme_etendu.A_ub,\n    b_ub=probleme_etendu.b_ub,\n    bounds=probleme_etendu.bounds,\n    method='highs'\n)\n\nif result_etendu.success:\n    x1_ext, x2_ext = result_etendu.x\n    benefice_ext = -result_etendu.fun\n    \n    print(\"\\n\" + \"=\" * 70)\n    print(\"ğŸ­ SOLUTION AVEC CONTRAINTES DE MATIÃˆRES PREMIÃˆRES\")\n    print(\"=\" * 70)\n    print(f\"\\nğŸ“¦ Production optimale :\")\n    print(f\"   â€¢ CanapÃ©s modÃ¨le A : {x1_ext:.0f} unitÃ©s\")\n    print(f\"   â€¢ CanapÃ©s modÃ¨le B : {x2_ext:.0f} unitÃ©s\")\n    print(f\"\\nğŸ’° BÃ©nÃ©fice maximal : {benefice_ext:,.0f} â‚¬\")\n    print(f\"\\nğŸ“‰ Comparaison avec le cas sans contraintes de matiÃ¨res :\")\n    print(f\"   â€¢ Perte de production : {(x1_opt + x2_opt) - (x1_ext + x2_ext):.0f} unitÃ©s\")\n    print(f\"   â€¢ Perte de bÃ©nÃ©fice : {max_benefice - benefice_ext:,.0f} â‚¬ ({100*(max_benefice - benefice_ext)/max_benefice:.1f}%)\")\n    \n    # Analyse des contraintes saturÃ©es\n    print(f\"\\nğŸ” Contraintes saturÃ©es (limitantes) :\")\n    utilisation = probleme_etendu.A_ub @ result_etendu.x\n    for i, (nom, util, dispo) in enumerate(zip(probleme_etendu.noms_contraintes, utilisation, probleme_etendu.b_ub)):\n        taux = 100 * util / dispo\n        if taux &gt; 99:\n            print(f\"   âš ï¸  {nom:12s} : {util:.1f} / {dispo:.0f} ({taux:.1f}%) - SATURÃ‰E\")\n        elif taux &gt; 90:\n            print(f\"   âš¡ {nom:12s} : {util:.1f} / {dispo:.0f} ({taux:.1f}%) - Presque saturÃ©e\")\n    \n    print(\"=\" * 70)\nelse:\n    print(\"âŒ ProblÃ¨me infaisable avec les contraintes de matiÃ¨res premiÃ¨res\")"
  },
  {
    "objectID": "files/Optimisation_Production_Complete.html#partie-6-tableau-de-bord-rÃ©capitulatif",
    "href": "files/Optimisation_Production_Complete.html#partie-6-tableau-de-bord-rÃ©capitulatif",
    "title": "ğŸ“Š Optimisation de Production - Programmation LinÃ©aire",
    "section": "ğŸ“Š Partie 6 : Tableau de bord rÃ©capitulatif",
    "text": "ğŸ“Š Partie 6 : Tableau de bord rÃ©capitulatif\n\n\nCode\n# CrÃ©ation d'un tableau comparatif\ncomparaison = pd.DataFrame({\n    'ScÃ©nario': ['Sans contraintes MP', 'Avec contraintes MP', 'Ã‰cart'],\n    'CanapÃ©s A': [x1_opt, x1_ext, x1_opt - x1_ext],\n    'CanapÃ©s B': [x2_opt, x2_ext, x2_opt - x2_ext],\n    'Total canapÃ©s': [x1_opt + x2_opt, x1_ext + x2_ext, (x1_opt + x2_opt) - (x1_ext + x2_ext)],\n    'BÃ©nÃ©fice (â‚¬)': [max_benefice, benefice_ext, max_benefice - benefice_ext]\n})\n\nprint(\"\\nğŸ“ˆ TABLEAU RÃ‰CAPITULATIF\")\nprint(\"=\" * 80)\ndisplay(comparaison.style.format({\n    'CanapÃ©s A': '{:.0f}',\n    'CanapÃ©s B': '{:.0f}',\n    'Total canapÃ©s': '{:.0f}',\n    'BÃ©nÃ©fice (â‚¬)': '{:,.0f}'\n}).background_gradient(subset=['BÃ©nÃ©fice (â‚¬)'], cmap='RdYlGn'))"
  },
  {
    "objectID": "files/Optimisation_Production_Complete.html#conclusions-et-recommandations",
    "href": "files/Optimisation_Production_Complete.html#conclusions-et-recommandations",
    "title": "ğŸ“Š Optimisation de Production - Programmation LinÃ©aire",
    "section": "ğŸ“ Conclusions et recommandations",
    "text": "ğŸ“ Conclusions et recommandations\n\nğŸ“Œ RÃ©sultats clÃ©s\n\nSans contraintes de matiÃ¨res premiÃ¨res :\n\nProduction optimale : 50 canapÃ©s A + 100 canapÃ©s B\nBÃ©nÃ©fice maximal : 35 000 â‚¬\nContrainte limitante : CapacitÃ© marchÃ© B\n\nAvec contraintes de matiÃ¨res premiÃ¨res :\n\nLa solution est plus contrainte\nPerte de bÃ©nÃ©fice due aux limitations de matiÃ¨res\nIdentification des goulots dâ€™Ã©tranglement\n\n\n\n\nğŸ’¡ Recommandations stratÃ©giques\n\nCourt terme :\n\nPrivilÃ©gier le modÃ¨le B (meilleur ratio bÃ©nÃ©fice/temps)\nNÃ©gocier avec fournisseurs pour les matiÃ¨res saturÃ©es\n\nMoyen terme :\n\nInvestir dans lâ€™augmentation des capacitÃ©s de production\nÃ‰tudier la possibilitÃ© dâ€™augmenter les prix du modÃ¨le A\n\nLong terme :\n\nDiversifier la gamme de produits\nOptimiser la chaÃ®ne logistique\n\n\n\n\nğŸ”§ Limites et extensions possibles\n\nHypothÃ¨ses simplificatrices : Production continue, pas de coÃ»ts fixes\nExtensions possibles :\n\nModÃ¨le stochastique avec incertitude sur la demande\nOptimisation multi-pÃ©riodes\nProgrammation en nombres entiers\nAnalyse coÃ»t-bÃ©nÃ©fice des investissements"
  },
  {
    "objectID": "files/Optimisation_Production_Complete.html#rÃ©fÃ©rences-et-ressources",
    "href": "files/Optimisation_Production_Complete.html#rÃ©fÃ©rences-et-ressources",
    "title": "ğŸ“Š Optimisation de Production - Programmation LinÃ©aire",
    "section": "ğŸ“š RÃ©fÃ©rences et ressources",
    "text": "ğŸ“š RÃ©fÃ©rences et ressources\n\nBibliothÃ¨ques utilisÃ©es\n\nNumPy : Calcul numÃ©rique et algÃ¨bre linÃ©aire\nSciPy : Optimisation (algorithme HiGHS)\nMatplotlib : Visualisation de donnÃ©es\nPandas : Manipulation et analyse de donnÃ©es\n\n\n\nMÃ©thodes\n\nProgrammation linÃ©aire : MÃ©thode du simplexe\nAnalyse de sensibilitÃ© : Ã‰tude paramÃ©trique\nVisualisation : MÃ©thode graphique\n\n\nNotebook rÃ©alisÃ© par Emmanuel Paguiel BOUENDO - Master MÃ‰cEn 2024-2025"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Parcours Professionnel",
    "section": "",
    "text": "Mon parcours\n    Ã‰tudiant en Master Ã‰conomiste dâ€™Entreprise\n    Formation actuelle : Ã©conomÃ©trie, data science et modÃ©lisation statistique (UniversitÃ© de Tours).\n  \n\n  \n    \n    \n    \n      \n      \n      \n        ğŸ’¼ ExpÃ©rience Professionnelle\n        \n        \n          \n          \n            \n              \n                \n                  Conseiller ClientÃ¨le\n                  SociÃ©tÃ© GÃ©nÃ©rale Congo\n                  \n                    ğŸ“ Brazzaville\n                    ğŸ“… 2019 â€“ 2022\n                  \n                \n              \n              \n                \n                  Gestion d'un portefeuille clients stratÃ©gique (+250 comptes/trimestre)\n                  Analyse CRM pour identifier les opportunitÃ©s de vente croisÃ©e\n                  Contribution aux campagnes nationales Better Together et Objectif RÃ©ussi\n                \n              \n            \n          \n          \n          \n            \n              \n                \n                  Gestionnaire de Production IARD\n                  Net Conseils\n                  \n                    ğŸ“ Brazzaville\n                    ğŸ“… 2018 â€“ 2019\n                    Stage\n                  \n                \n              \n              \n                Gestion administrative des polices d'assurance et participation aux campagnes de prospection.\n              \n            \n          \n\n          \n            \n              \n                \n                  Middle Office Risques\n                  La Congolaise de Banque\n                  \n                    ğŸ“ Brazzaville\n                    ğŸ“… 2016 â€“ 2017\n                    Stage\n                  \n                \n              \n              \n                Analyse des dossiers de crÃ©dit, reporting sur les risques et suivi des indicateurs de performance.\n              \n            \n          \n\n        \n      \n\n      \n      \n        ğŸ† RÃ©alisations\n        \n        \n          \n          \n            ğŸ–ï¸\n            DiplÃ´me de SuccÃ¨s\n            Reconnaissance pour performance commerciale exceptionnelle\n            ğŸ“„ Voir le certificat\n          \n\n          \n            ğŸ¥‡\n            Better Together\n            +15% des ouvertures de compte lors de la campagne nationale\n            ğŸ“„ Voir le certificat\n          \n\n          \n            ğŸš€\n            Objectif RÃ©ussi\n            250 ouvertures de compte par trimestre\n            ğŸ“„ Voir le certificat\n          \n\n        \n      \n\n    \n\n    \n    \n      \n      \n      \n        ğŸ“ Formation\n        \n        \n          \n          \n            \n              \n                \n                  Master Ã‰conomiste d'Entreprise\n                  UniversitÃ© de Tours\n                  \n                    ğŸ“ Tours, France\n                    ğŸ“… 2024 â€“ 2026\n                  \n                \n                \n              \n              \n                SpÃ©cialisation Data Science appliquÃ©e Ã  l'Ã©conomie : Machine Learning, Ã‰conomÃ©trie avancÃ©e, Big Data.\n              \n              ğŸ”— En savoir plus\n            \n          \n\n          \n            \n              \n                \n                  Licence Ã‰conomie de l'Entreprise\n                  UniversitÃ© de Tours\n                  \n                    ğŸ“ Tours, France\n                    ğŸ“… 2023 â€“ 2024\n                  \n                \n                \n              \n              ğŸ”— En savoir plus\n            \n          \n          \n          \n            \n              \n                \n                  Master Ã‰conomie et Organisation\n                  UniversitÃ© Marien Ngouabi\n                  \n                    ğŸ“ Brazzaville, Congo\n                    ğŸ“… 2019 â€“ 2021\n                  \n                \n                \n              \n              ğŸ”— En savoir plus\n            \n          \n          \n          \n            \n              \n                \n                  Licence Ã‰conomie de l'Entreprise\n                  UniversitÃ© Marien Ngouabi\n                  \n                    ğŸ“ Brazzaville, Congo\n                    ğŸ“… 2015 â€“ 2018\n                  \n                \n              \n            \n          \n\n        \n      \n\n      \n      \n\n  ğŸ“š Formation en cours\n  \n  \n    \n      ğŸ“Š\n      \n        Google Data Analytics\n        Coursera â€¢ Google â€¢ En cours\n      \n    \n    \n      ğŸ¤–\n      \n        Machine Learning (Stanford)\n        Coursera â€¢ Andrew Ng â€¢ Ã€ venir"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog Technique",
    "section": "",
    "text": "Bienvenue sur mon blog technique ! Ici, je partage mes rÃ©flexions, dÃ©couvertes et retours dâ€™expÃ©rience sur la data science, le machine learning et les technologies Ã©mergentes.\n\n\n\n\nGuide complet pour passer de Jupyter Notebook Ã  une API robuste en production avec FastAPI, Docker et AWS.\nTopics : MLOps, Docker, AWS, FastAPI\n\n\n\n\nRetour dâ€™expÃ©rience sur les erreurs courantes en preprocessing et comment les Ã©viter pour des modÃ¨les plus performants.\nTopics : Feature Engineering, Preprocessing, Best Practices\n\n\n\n\nExploration des outils modernes de dataviz : Plotly, Altair, et techniques dâ€™interactivitÃ© avancÃ©es.\nTopics : Visualisation, Plotly, Interactive Charts\n\n\n\n\n\n\n\nSÃ©rie complÃ¨te sur la mise en production de modÃ¨les ML : 1. Architecture et bonnes pratiques 2. Monitoring et maintenance 3. CI/CD pour le Machine Learning 4. Gestion des versions de modÃ¨les\n\n\n\nGuide pratique pour maÃ®triser lâ€™Ã©cosystÃ¨me Python data : 1. Pandas avancÃ© et optimisation 2. Numpy et calcul vectoriel 3. Scikit-learn : trucs et astuces 4. IntÃ©gration avec le Big Data\n\n\n\nRÃ©flexions sur les enjeux Ã©thiques : 1. DÃ©tection et mitigation des biais 2. ExplicabilitÃ© des modÃ¨les 3. Protection de la vie privÃ©e 4. Impact sociÃ©tal de lâ€™IA\n\n\n\n\n\n\nIA GÃ©nÃ©rative : Applications pratiques des LLMs en entreprise\nAutoML : Quand et comment automatiser le ML\nEdge AI : DÃ©ploiement de modÃ¨les sur IoT\nQuantum ML : Lâ€™avenir du machine learning quantique\n\n\n\n\n\n\n\n\nPyData Paris : â€œMLOps : de lâ€™expÃ©rimentation Ã  la productionâ€\nDataCon : â€œÃ‰thique et biais en IA : guide pratiqueâ€\nAI Summit : â€œROI et mesure dâ€™impact des projets dataâ€\n\n\n\n\n\nDataCast : â€œParcours dâ€™un data scientistâ€ (45min)\nML Weekly : â€œTrends en 2024â€ (30min)\nFrench Data : â€œOutils open source incontournablesâ€ (1h)\n\n\n\n\n\n\n\n\n\nâ€œHands-On Machine Learningâ€ - AurÃ©lien GÃ©ron\nâ€œThe Elements of Statistical Learningâ€ - Hastie, Tibshirani, Friedman\nâ€œPython for Data Analysisâ€ - Wes McKinney\nâ€œBuilding Machine Learning Powered Applicationsâ€ - Emmanuel Ameisen\n\n\n\n\n\nThe Batch (Andrew Ng)\nTowards Data Science (Medium)\nKDnuggets\nDistill.pub (pour les aspects thÃ©oriques)\n\n\n\n\n\nFast.ai : Practical Deep Learning\nCoursera : ML Specialization (Stanford)\nedX : MIT Introduction to Computational Thinking\n\n\n\n\n\n\nJe suis ouvert aux collaborations ! Si vous souhaitez : - Guest post sur mon blog - Collaboration sur un article - Interview ou podcast - Revue technique de votre contenu\nNâ€™hÃ©sitez pas Ã  me contacter !\n\n\n\n\nğŸ“§ votre.email@exemple.com ğŸ’¼ LinkedIn ğŸ™ GitHub ğŸ¦ Twitter\n\n\nInscrivez-vous Ã  ma newsletter mensuelle pour recevoir : - RÃ©sumÃ©s des derniers articles - SÃ©lection dâ€™outils et ressources - Trends et actualitÃ©s data science - Invitations Ã  mes talks et webinars\nğŸ“® Sâ€™inscrire Ã  la newsletter\n\nBlog alimentÃ© par la passion de partager et dâ€™apprendre ensemble ! ğŸš€"
  },
  {
    "objectID": "blog.html#articles-rÃ©cents",
    "href": "blog.html#articles-rÃ©cents",
    "title": "Blog Technique",
    "section": "",
    "text": "Guide complet pour passer de Jupyter Notebook Ã  une API robuste en production avec FastAPI, Docker et AWS.\nTopics : MLOps, Docker, AWS, FastAPI\n\n\n\n\nRetour dâ€™expÃ©rience sur les erreurs courantes en preprocessing et comment les Ã©viter pour des modÃ¨les plus performants.\nTopics : Feature Engineering, Preprocessing, Best Practices\n\n\n\n\nExploration des outils modernes de dataviz : Plotly, Altair, et techniques dâ€™interactivitÃ© avancÃ©es.\nTopics : Visualisation, Plotly, Interactive Charts"
  },
  {
    "objectID": "blog.html#sÃ©ries-Ã -venir",
    "href": "blog.html#sÃ©ries-Ã -venir",
    "title": "Blog Technique",
    "section": "",
    "text": "SÃ©rie complÃ¨te sur la mise en production de modÃ¨les ML : 1. Architecture et bonnes pratiques 2. Monitoring et maintenance 3. CI/CD pour le Machine Learning 4. Gestion des versions de modÃ¨les\n\n\n\nGuide pratique pour maÃ®triser lâ€™Ã©cosystÃ¨me Python data : 1. Pandas avancÃ© et optimisation 2. Numpy et calcul vectoriel 3. Scikit-learn : trucs et astuces 4. IntÃ©gration avec le Big Data\n\n\n\nRÃ©flexions sur les enjeux Ã©thiques : 1. DÃ©tection et mitigation des biais 2. ExplicabilitÃ© des modÃ¨les 3. Protection de la vie privÃ©e 4. Impact sociÃ©tal de lâ€™IA"
  },
  {
    "objectID": "blog.html#sujets-que-jaimerais-explorer",
    "href": "blog.html#sujets-que-jaimerais-explorer",
    "title": "Blog Technique",
    "section": "",
    "text": "IA GÃ©nÃ©rative : Applications pratiques des LLMs en entreprise\nAutoML : Quand et comment automatiser le ML\nEdge AI : DÃ©ploiement de modÃ¨les sur IoT\nQuantum ML : Lâ€™avenir du machine learning quantique"
  },
  {
    "objectID": "blog.html#interventions-et-confÃ©rences",
    "href": "blog.html#interventions-et-confÃ©rences",
    "title": "Blog Technique",
    "section": "",
    "text": "PyData Paris : â€œMLOps : de lâ€™expÃ©rimentation Ã  la productionâ€\nDataCon : â€œÃ‰thique et biais en IA : guide pratiqueâ€\nAI Summit : â€œROI et mesure dâ€™impact des projets dataâ€\n\n\n\n\n\nDataCast : â€œParcours dâ€™un data scientistâ€ (45min)\nML Weekly : â€œTrends en 2024â€ (30min)\nFrench Data : â€œOutils open source incontournablesâ€ (1h)"
  },
  {
    "objectID": "blog.html#ressources-recommandÃ©es",
    "href": "blog.html#ressources-recommandÃ©es",
    "title": "Blog Technique",
    "section": "",
    "text": "â€œHands-On Machine Learningâ€ - AurÃ©lien GÃ©ron\nâ€œThe Elements of Statistical Learningâ€ - Hastie, Tibshirani, Friedman\nâ€œPython for Data Analysisâ€ - Wes McKinney\nâ€œBuilding Machine Learning Powered Applicationsâ€ - Emmanuel Ameisen\n\n\n\n\n\nThe Batch (Andrew Ng)\nTowards Data Science (Medium)\nKDnuggets\nDistill.pub (pour les aspects thÃ©oriques)\n\n\n\n\n\nFast.ai : Practical Deep Learning\nCoursera : ML Specialization (Stanford)\nedX : MIT Introduction to Computational Thinking"
  },
  {
    "objectID": "blog.html#collaboration-et-guest-posts",
    "href": "blog.html#collaboration-et-guest-posts",
    "title": "Blog Technique",
    "section": "",
    "text": "Je suis ouvert aux collaborations ! Si vous souhaitez : - Guest post sur mon blog - Collaboration sur un article - Interview ou podcast - Revue technique de votre contenu\nNâ€™hÃ©sitez pas Ã  me contacter !"
  },
  {
    "objectID": "blog.html#restons-en-contact",
    "href": "blog.html#restons-en-contact",
    "title": "Blog Technique",
    "section": "",
    "text": "ğŸ“§ votre.email@exemple.com ğŸ’¼ LinkedIn ğŸ™ GitHub ğŸ¦ Twitter\n\n\nInscrivez-vous Ã  ma newsletter mensuelle pour recevoir : - RÃ©sumÃ©s des derniers articles - SÃ©lection dâ€™outils et ressources - Trends et actualitÃ©s data science - Invitations Ã  mes talks et webinars\nğŸ“® Sâ€™inscrire Ã  la newsletter\n\nBlog alimentÃ© par la passion de partager et dâ€™apprendre ensemble ! ğŸš€"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "index.html#derniers-projets",
    "href": "index.html#derniers-projets",
    "title": "",
    "section": "ğŸ¯ Derniers Projets",
    "text": "ğŸ¯ Derniers Projets\n\nVoici quelques travaux rÃ©cents.\n\n\n\n  \n  \n    \n      ğŸ’¼ PrÃ©dicteur de Salaires Data Jobs\n    \n\n    \n      \n    \n\n    \n       Ce projet vise Ã  estimer les salaires dans les mÃ©tiers de la data Ã  partir de 5 868 offres HelloWork. \n    Il repose sur un modÃ¨le XGBoost (RÂ² = 0.337) et permet dâ€™explorer les niveaux de rÃ©munÃ©ration selon lâ€™expÃ©rience, les compÃ©tences et la localisation, ainsi quâ€™une feuille de route carriÃ¨re personnalisÃ©e.\n      \n      \n        Python\n        XGBoost\n        Streamlit\n        Machine Learning\n      \n    \n\n    \n      En savoir plus\n      Code source\n    \n  \n\n  \n  \n    \n      ğŸ¤– Ã‰valuation des modÃ¨les de prÃ©diction du Bonheur\n    \n\n    \n      \n    \n\n    \n       Ce projet sâ€™interroge sur les dÃ©terminants du bonheur dÃ©clarÃ© Ã  partir de donnÃ©es dâ€™enquÃªte. \n    En comparant plusieurs algorithmes de classification (arbre de dÃ©cision, forÃªt alÃ©atoire, SVM, etc.), il cherche Ã  identifier les facteurs socio-Ã©conomiques les plus associÃ©s au niveau de bien-Ãªtre subjectif, et Ã  Ã©valuer la performance prÃ©dictive de ces modÃ¨les.\n      \n      \n        R\n        Classification\n        Random Forest\n        SVM\n      \n    \n\n    \n      En savoir plus\n    \n  \n\n  \n  \n    \n      ğŸ“Š Dashboard Interactif CriminalitÃ©\n    \n\n    \n      \n    \n\n    \n       Ce projet vise Ã  mieux comprendre la concentration et lâ€™Ã©volution des faits criminels au niveau local, en croisant trois dimensions : rÃ©partition gÃ©ographique, typologie des infractions et Ã©volution temporelle. \n    Lâ€™application, construite Ã  partir des donnÃ©es officielles du MinistÃ¨re de lâ€™IntÃ©rieur, est accessible en ligne et publiÃ©e sur data.gouv.fr.\n      \n      \n        R Shiny\n        Visualisation\n        Cartographie\n        Analyse spatiale\n      \n    \n\n    \n      Voir l'application\n    \n  \n\n\n\n\nCompÃ©tences et contact\nPour en savoir plus sur mes compÃ©tences ou me contacter, consultez la page dÃ©diÃ©e ou Ã©crivez-moi directement.\n\nMes CompÃ©tences Me Contacter"
  },
  {
    "objectID": "porto/happy_prediction.html",
    "href": "porto/happy_prediction.html",
    "title": "Ã‰valuation des ModÃ¨les de PrÃ©diction du Bonheur",
    "section": "",
    "text": "EP Bouendo\n    \n      Accueil\n      Projets\n      Ã€ propos\n      Contact"
  },
  {
    "objectID": "porto/happy_prediction.html#rÃ©sumÃ©",
    "href": "porto/happy_prediction.html#rÃ©sumÃ©",
    "title": "Ã‰valuation des ModÃ¨les de PrÃ©diction du Bonheur",
    "section": "RÃ©sumÃ©",
    "text": "RÃ©sumÃ©\n\nCe travail explore les liens entre bien-Ãªtre subjectif et variables socio-Ã©conomiques (revenu, emploi, santÃ©, lien social) Ã  partir de donnÃ©es d'enquÃªte. PlutÃ´t que de chercher Ã  Â« prÃ©dire Â» le bonheur, l'objectif est d'identifier quels facteurs sont les plus fortement associÃ©s au niveau de bien-Ãªtre dÃ©clarÃ©. Plusieurs algorithmes de classification (arbre de dÃ©cision, forÃªt alÃ©atoire, SVM, etc.) sont comparÃ©s afin d'Ã©valuer leur capacitÃ© Ã  capturer ces associations.\n\n\n  01\n  Objectifs de l'Analyse\n\n\n  \n    1Comprendre les liens\n    Analyser comment les variables socio-Ã©conomiques co-varient avec le bien-Ãªtre subjectif\n  \n  \n  \n    2PrÃ©traitement adaptÃ©\n    Adapter le nettoyage et l'encodage aux spÃ©cificitÃ©s des donnÃ©es d'enquÃªte\n  \n  \n  \n    3Comparer les modÃ¨les\n    Ã‰valuer la stabilitÃ© et la cohÃ©rence des rÃ©sultats selon les algorithmes\n  \n  \n  \n    4InterprÃ©ter les signaux\n    Identifier les variables les plus informatives, indÃ©pendamment de la performance brute\n  \n\n\nDÃ©marche mÃ©thodologique\nL'analyse repose sur un partitionnement stratifiÃ©, une validation croisÃ©e (10-fold), et une Ã©valuation sur un ensemble de test indÃ©pendant. L'objectif n'est pas d'optimiser la performance maximale, mais de comparer la performance des signaux produits par diffÃ©rents algorithmes.\n\n\n  02\n  Indicateurs de ProcÃ©dure\n\n\n  \n    07\n    Algorithmes testÃ©s\n  \n  \n  \n    05\n    MÃ©triques d'Ã©valuation\n  \n  \n  \n    10-Fold\n    Validation croisÃ©e\n  \n  \n  \n    95%\n    Intervalle de confiance\n  \n\n\n  04\n  Observations Principales\n\n\n\n\n\n\n\nNoneCe que ce projet permet dâ€™observer\n\n\n\n\nQuelles variables socio-Ã©conomiques sont le plus souvent associÃ©es au bien-Ãªtre dÃ©clarÃ©\nComment les choix algorithmiques influencent la stabilitÃ© des conclusions\nLes limites de lâ€™infÃ©rence Ã  partir de donnÃ©es auto-dÃ©clarÃ©es\n\n\n\n\n\n\n\n\n\nNoneMÃ©triques dâ€™Ã‰valuation\n\n\n\nLâ€™Ã©valuation des modÃ¨les sâ€™appuie sur un ensemble complet de mÃ©triques : AUC-ROC, F1-Score, PrÃ©cision, Rappel, et Accuracy, garantissant une analyse Ã©quilibrÃ©e des performances prÃ©dictives."
  },
  {
    "objectID": "porto/salary_predictor.html",
    "href": "porto/salary_predictor.html",
    "title": "PrÃ©dicteur de Salaires Data Jobs",
    "section": "",
    "text": "â† Retour Ã  lâ€™accueil â€¢ Tous les projets"
  },
  {
    "objectID": "porto/salary_predictor.html#contexte",
    "href": "porto/salary_predictor.html#contexte",
    "title": "PrÃ©dicteur de Salaires Data Jobs",
    "section": "Contexte",
    "text": "Contexte\nCe projet explore les dÃ©terminants des salaires dans les mÃ©tiers de la Data Ã  partir dâ€™un Ã©chantillon de 5 868 offres dâ€™emploi HelloWork (janvier 2026). Un modÃ¨le de machine learning (XGBoost) a Ã©tÃ© entraÃ®nÃ© pour estimer la rÃ©munÃ©ration en fonction de variables observables (expÃ©rience, compÃ©tences, localisation, secteur). Lâ€™application Streamlit qui en dÃ©coule permet dâ€™explorer les tendances observÃ©es, dâ€™interprÃ©ter les contributions des variables (via SHAP) et de se positionner relativement Ã  lâ€™Ã©chantillon sans prÃ©tendre Ã  une estimation dÃ©finitive."
  },
  {
    "objectID": "porto/salary_predictor.html#dataset",
    "href": "porto/salary_predictor.html#dataset",
    "title": "PrÃ©dicteur de Salaires Data Jobs",
    "section": "Dataset",
    "text": "Dataset\n\nSource : HelloWork (janvier 2026)\nTaille : 5 868 offres dâ€™emploi\nÃ‰chantillon entraÃ®nement : 2 681 offres\nMÃ©tiers : Data Analyst, Data Scientist, Data Engineer, BI/Analytics\nZones : France entiÃ¨re (17 villes principales)"
  },
  {
    "objectID": "porto/salary_predictor.html#modÃ¨le-ml",
    "href": "porto/salary_predictor.html#modÃ¨le-ml",
    "title": "PrÃ©dicteur de Salaires Data Jobs",
    "section": "ModÃ¨le ML",
    "text": "ModÃ¨le ML\nAlgorithme : XGBoost v7\nPerformance :\n\nRÂ² = 0.337 : le modÃ¨le explique environ un tiers de la variance observÃ©e, ce qui est cohÃ©rent avec la complexitÃ© des dÃ©terminants salariaux (facteurs non observÃ©s, nÃ©gociation individuelle, asymÃ©tries dâ€™information, etc.)\nMAE = 5 163 â‚¬ (erreur moyenne absolue)\nRMSE = 6 969 â‚¬ (erreur quadratique moyenne)\nPrÃ©cision Â±15% = 73.7% (3 prÃ©dictions sur 4 dans la fourchette)\nPrÃ©cision Â±10% = 57.9%\n\nFeatures : 29 variables incluant expÃ©rience, localisation, secteur, compÃ©tences techniques, tÃ©lÃ©travail"
  },
  {
    "objectID": "porto/salary_predictor.html#fonctionnalitÃ©s",
    "href": "porto/salary_predictor.html#fonctionnalitÃ©s",
    "title": "PrÃ©dicteur de Salaires Data Jobs",
    "section": "FonctionnalitÃ©s",
    "text": "FonctionnalitÃ©s\n\n1. Estimation personnalisÃ©e\n\nFormulaire de saisie de profil (11 champs)\nEstimation salariale basÃ©e sur le modÃ¨le XGBoost\nExplication SHAP (contribution des variables)\nPositionnement relatif (percentile par rapport Ã  lâ€™Ã©chantillon)\n\n\n\n2. Analyse du marchÃ©\n\nDistributions salariales par poste et secteur\nComparaisons gÃ©ographiques\nImpact des compÃ©tences techniques sur la rÃ©munÃ©ration\nFiltres dynamiques (ville, secteur, expÃ©rience)\n\n\n\n3. Feuille de route carriÃ¨re\n\nAnalyse du positionnement actuel\nSuggestions de compÃ©tences associÃ©es Ã  des niveaux de rÃ©munÃ©ration plus Ã©levÃ©s\nComparaison avec des profils similaires\nSimulation de transitions de rÃ´le (ex: Data Analyst â†’ Data Scientist)"
  },
  {
    "objectID": "porto/salary_predictor.html#stack-technique",
    "href": "porto/salary_predictor.html#stack-technique",
    "title": "PrÃ©dicteur de Salaires Data Jobs",
    "section": "ï¸ Stack technique",
    "text": "ï¸ Stack technique\n# Frontend\nstreamlit==1.31.0\nplotly==5.18.0\nmatplotlib==3.8.2\n\n# ML\nxgboost==2.0.3\nscikit-learn==1.3.2\nshap==0.44.0\n\n# Data\npandas==2.1.4\nnumpy==1.26.2"
  },
  {
    "objectID": "porto/salary_predictor.html#aperÃ§u-de-lapplication",
    "href": "porto/salary_predictor.html#aperÃ§u-de-lapplication",
    "title": "PrÃ©dicteur de Salaires Data Jobs",
    "section": "AperÃ§u de lâ€™application",
    "text": "AperÃ§u de lâ€™application\n\n\n\nPage dâ€™accueil\n\n\n\n\n\nPrÃ©diction\n\n\n\n\n\nAnalyse marchÃ©\n\n\n\n\n\nFeuille de route"
  },
  {
    "objectID": "porto/salary_predictor.html#dÃ©ploiement",
    "href": "porto/salary_predictor.html#dÃ©ploiement",
    "title": "PrÃ©dicteur de Salaires Data Jobs",
    "section": "DÃ©ploiement",
    "text": "DÃ©ploiement\nLâ€™application est dÃ©ployÃ©e sur Streamlit Cloud :\n\nHÃ©bergement : Streamlit Cloud\nBackend : Python 3.13\nVersioning : GitHub\nDisponibilitÃ© : En ligne 24/7"
  },
  {
    "objectID": "porto/salary_predictor.html#dÃ©fis-techniques-rencontrÃ©s",
    "href": "porto/salary_predictor.html#dÃ©fis-techniques-rencontrÃ©s",
    "title": "PrÃ©dicteur de Salaires Data Jobs",
    "section": "DÃ©fis techniques rencontrÃ©s",
    "text": "DÃ©fis techniques rencontrÃ©s\n\nFeature engineering : extraction et encodage de 29 variables Ã  partir de textes bruts\nGestion des chemins : Adaptation pour dÃ©ploiement cloud\nOptimisation : mise en cache des modÃ¨les et donnÃ©es via @st.cache_data\nExplicabilitÃ© : intÃ©gration de SHAP pour rendre les prÃ©dictions interprÃ©tables"
  },
  {
    "objectID": "porto/salary_predictor.html#liens",
    "href": "porto/salary_predictor.html#liens",
    "title": "PrÃ©dicteur de Salaires Data Jobs",
    "section": "ğŸ”— Liens",
    "text": "ğŸ”— Liens\n\nApplication en ligne\nCode source GitHub\nDocumentation\n\n\n\nâ† Accueil â€¢ Tous les projets"
  },
  {
    "objectID": "projects/ecommerce.html",
    "href": "projects/ecommerce.html",
    "title": "ModÃ©lisation PrÃ©dictive du Marketing Bancaire",
    "section": "",
    "text": "â† Retour aux projets\n\n  \n  \n    Marketing Bancaire\n    ModÃ©lisation PrÃ©dictive du Marketing Bancaire\n    Analyse dâ€™une campagne de tÃ©lÃ©marketing au Portugal (2008â€“2010) combinant Random Forest, interprÃ©tation SHAP et segmentation client.\n  \n\n  \n  \n    \n      ğŸ™ Code Source\n    \n    \n      ğŸŒ Application Live\n    \n    \n      ğŸ“¥ Rapport Technique (PDF)\n    \n  \n\n  \n  \n    \n      41 188\n      Clients\n    \n    \n      11.3%\n      Taux de conversion\n    \n    \n      0.945\n      AUC (Test)\n    \n    \n      4\n      Segments clients\n    \n  \n\n  \n  \n    Contexte & Objectifs\n    \n      \n        Ce projet analyse une campagne de tÃ©lÃ©marketing menÃ©e par une banque portugaise entre 2008 et 2010, portant sur 41â€¯188 contacts clients.\n        Lâ€™objectif est dâ€™explorer les facteurs associÃ©s Ã  la souscription Ã  un dÃ©pÃ´t Ã  terme.\n      \n      \n        La dÃ©marche vise Ã  :\n      \n      \n        Comparer plusieurs algorithmes de classification via le framework tidymodels\n        InterprÃ©ter les dÃ©cisions du modÃ¨le retenu Ã  lâ€™aide de lâ€™analyse SHAP\n        Segmenter la clientÃ¨le par K-means pour identifier des profils homogÃ¨nes\n        Concevoir un systÃ¨me de scoring intÃ©grÃ© Ã  lâ€™application\n      \n    \n  \n\n  \n  \n    MÃ©thodologie\n    \n      \n        Afin dâ€™Ã©viter les biais courants en modÃ©lisation prÃ©dictive, les Ã©tapes suivantes ont Ã©tÃ© respectÃ©es :\n      \n      \n        \n          SÃ©paration initiale\n          Division train/test (80/20) rÃ©alisÃ©e avant tout traitement des donnÃ©es\n        \n        \n          PrÃ©traitement sÃ©curisÃ©\n          Imputation, encodage et normalisation estimÃ©s uniquement sur lâ€™Ã©chantillon dâ€™entraÃ®nement\n        \n        \n          RÃ©Ã©quilibrage contrÃ´lÃ©\n          La mÃ©thode SMOTE a Ã©tÃ© appliquÃ©e uniquement au sein de chaque pli de validation croisÃ©e\n        \n        \n          Ã‰valuation finale\n          Performance mesurÃ©e sur lâ€™Ã©chantillon de test, jamais utilisÃ© durant lâ€™entraÃ®nement\n        \n      \n    \n  \n\n  \n  \n    RÃ©sultats du modÃ¨le\n    \n      \n        Meilleur algorithme\n        Le Random Forest obtient la meilleure performance (AUC = 0.945), surpassant XGBoost, les SVM et les rÃ©seaux de neurones.\n      \n      \n        Variables influentes\n        La durÃ©e de lâ€™appel, le taux dâ€™emploi local et lâ€™indice de confiance des consommateurs sont les variables les plus discriminantes.\n      \n      \n        Segments clients\n        Quatre segments ont Ã©tÃ© identifiÃ©s, dont lâ€™un prÃ©sente un taux de conversion Ã©levÃ© (46 %) malgrÃ© un contexte Ã©conomique dÃ©favorable.\n      \n    \n  \n\n  \n  \n    SystÃ¨me de scoring\n    \n      \n        Un systÃ¨me de scoring a Ã©tÃ© conÃ§u et intÃ©grÃ© Ã  lâ€™application, permettant dâ€™estimer la probabilitÃ© de conversion Ã  partir des caractÃ©ristiques client saisies.\n      \n      \n        Calcul de la probabilitÃ© via le workflow entraÃ®nÃ©\n        Affichage des contributions SHAP pour interprÃ©tabilitÃ© locale\n        Classification en catÃ©gories de prioritÃ© (trÃ¨s Ã©levÃ©e Ã  trÃ¨s faible)\n        Suggestions contextuelles (ex. : prolonger un appel court)\n      \n    \n  \n\n  \n\n  Visualisations\n\n  \n\n    \n      \n        \n      \n    \n\n    \n      \n        \n      \n    \n\n    \n      \n        \n      \n    \n\n    \n      \n        \n      \n    \n\n  \n\n\n  \n  \n    Stack technologique\n    \n      R 4.3+\n      tidymodels\n      ranger\n      fastshap\n      themis\n      ggplot2\n      patchwork\n      Shiny\n    \n  \n\n  \n  \n    Observations\n    \n      \n        Le segment Ã  fort potentiel se caractÃ©rise par des appels longs, un Ã¢ge plus Ã©levÃ© et un contexte Ã©conomique tendu\n        Un segment montre une sollicitation Ã©levÃ©e (13 contacts en moyenne) mais une conversion faible (4 %)\n        Abaisser le seuil de dÃ©cision de 0,5 Ã  0,1 permettrait thÃ©oriquement dâ€™augmenter le profit simulÃ© de 78 %\n        Les mois de juillet et septembre, ainsi que le canal cellulaire, sont associÃ©s Ã  de meilleures performances\n      \n    \n  \n\n  \n  \n    Conclusion\n    \n      \n        Ce projet utilise la modÃ©lisation prÃ©dictive et lâ€™analyse SHAP pour examiner les facteurs associÃ©s Ã  la souscription lors dâ€™une campagne de tÃ©lÃ©marketing.\n        Les rÃ©sultats montrent que la probabilitÃ© de conversion dÃ©pend Ã  la fois des caractÃ©ristiques individuelles, du contexte socio-Ã©conomique local, et de la nature de lâ€™interaction (durÃ©e, canal)."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\n\n  \n  \n    Projets\n    Analyses croisant Ã©conomie, statistique et apprentissage automatique\n  \n\n  \n    Tous\n    Machine Learning\n    Visualisation\n    Ã‰conomie\n    Data Engineering\n    Python\n  \n\n  \n\n    \n\n  \n    Machine Learning\n    âœ“ TerminÃ©\n  \n  \n    ModÃ©lisation PrÃ©dictive du Marketing Bancaire\n    Analyse exploratoire dâ€™une campagne de tÃ©lÃ©marketing bancaire au Portugal (2008â€“2010, 41 000 clients) pour comprendre quels profils sont les plus enclins Ã  souscrire un dÃ©pÃ´t Ã  terme. AprÃ¨s comparaison de plusieurs algorithmes (logistique, arbre de dÃ©cision, forÃªt alÃ©atoire, etc.), le modÃ¨le Random Forest sâ€™est distinguÃ© par sa performance (AUC = 0,944) et a Ã©tÃ© retenu pour estimer la propension Ã  la souscription.\n    \n      R\n      tidymodels\n      SHAP\n      K-Means\n    \n    Voir le projet â†’\n  \n\n\n  \n\n  \n    Analyse quantitative\n    âœ“ DÃ©ployÃ©\n  \n  \n    CrashAlert â€“ Analyse des accidents corporels de la circulation (2015â€“2024)\n    \n     Ce projet explore les facteurs associÃ©s aux accidents corporels de la circulation en France Ã  partir des donnÃ©es officielles de la base BAAC (ONISR). Lâ€™application permet dâ€™observer les variations gÃ©ographiques, temporelles et contextuelles (infrastructure, conditions mÃ©tÃ©orologiques, caractÃ©ristiques des usagers), ainsi que des indicateurs agrÃ©gÃ©s par territoire et pÃ©riode.\n    \n    \n      R Shiny\n      DonnÃ©es BAAC\n      Statistique descriptive\n      Open Data\n    \n    AccÃ©der Ã  l'application â†’\n  \n\n\n    \n    \n      \n        Machine Learning\n        âœ“ TerminÃ©\n      \n      \n        Ã‰valuation Comparative des ModÃ¨les de PrÃ©diction du Bonheur\n        Ce projet explore les liens entre bien-Ãªtre subjectif et variables socio-Ã©conomiques Ã  partir de donnÃ©es dâ€™enquÃªte. Plusieurs algorithmes de classification sont comparÃ©s afin dâ€™Ã©valuer leur capacitÃ© Ã  capturer ces associations.\n        \n          R\n          Random Forest\n          SVM\n          Validation croisÃ©e\n        \n        Voir le projet â†’\n      \n    \n\n    \n    \n      \n        Visualisation\n        âœ“ TerminÃ©\n      \n      \n        CrimStream Dashboard\n        Ce projet explore la concentration et lâ€™Ã©volution des faits criminels au niveau local, en croisant trois dimensions : rÃ©partition gÃ©ographique, typologie des infractions et Ã©volution temporelle. Lâ€™application, construite Ã  partir des donnÃ©es officielles du MinistÃ¨re de lâ€™IntÃ©rieur, est accessible en ligne et publiÃ©e sur data.gouv.fr.\n        \n          R Shiny\n          ggplot2\n          DT\n        \n        Voir le projet â†’\n      \n    \n\n    \n\n  \n    Ã‰conomie\n    âœ“ TerminÃ©\n  \n  \n    Performance Sectorielle en Afrique\n    Cette Ã©tude analyse les dÃ©terminants de la performance sectorielle en Afrique subsaharienne Ã  partir dâ€™un panel couvrant 11 pays et 12 secteurs sur la pÃ©riode 1970â€“2010. Elle examine Ã  la fois la persistance des Ã©carts de niveau et les facteurs associÃ©s Ã  la croissance sectorielle.\n    \n      R\n      DonnÃ©es de panel\n      fixest / plm\n    \n    Voir le projet â†’\n  \n\n\n   \n\n  \n    Ã‰conomie\n    âœ“ TerminÃ©\n  \n  \n    DÃ©terminants de l'emploi non salariÃ©\n    \n      Ce mÃ©moire analyse les facteurs structurels et conjoncturels associÃ©s Ã  lâ€™emploi non salariÃ© en France en 2015. La dÃ©marche combine analyse descriptive, ACM (analyse des correspondances multiples), classification par K-means et modÃ©lisation probit, Ã  partir de 198 234 observations issues de lâ€™EnquÃªte Emploi en Continu (EEC) de lâ€™INSEE.\n    \n    \n      R\n      ACM\n      K-means\n      Probit\n      EEC 2015\n    \n    Voir le mÃ©moire â†’\n  \n\n\n    \n    \n      \n        Ã‰conomie\n        ğŸ”„ En cours\n      \n      \n        ModÃ©lisation actuarielle en assurance automobile\n         Analyse actuarielle appliquÃ©e Ã  un portefeuille dâ€™assurance automobile, avec modÃ©lisation de la frÃ©quence et de la sÃ©vÃ©ritÃ© des sinistres Ã  lâ€™aide de GLM (Binomial NÃ©gatif, loi log-normale). \n        \n          R\n          GLM\n          Actuariat\n          freMTPL2\n          Ã‰conomÃ©trie\n        \n        Voir le projet â†’\n      \n    \n\n    \n\n  \n    Data\n    âœ“ TerminÃ©\n  \n  \n    SQL - Base Musicale\n    \n      Ce projet met en Å“uvre des techniques de requÃªtage et de modÃ©lisation sur une base de donnÃ©es musicale (inspirÃ©e de Chinook). Il comprend la crÃ©ation de vues, lâ€™utilisation de CTE, le dÃ©veloppement de procÃ©dures stockÃ©es et de fonctions personnalisÃ©es, ainsi que des agrÃ©gations multi-niveaux pour le reporting automatisÃ© notamment lâ€™analyse des ventes par pays et la segmentation des clients.\n    \n    \n      MySQL\n      Vues SQL\n      CTE\n      ProcÃ©dures\n      Fonctions\n      AgrÃ©gations\n    \n    Voir le projet â†’\n  \n\n\n\n\n  \n    Python\n    âœ“ TerminÃ©\n  \n  \n    Optimisation de Production\n    \n      Ce projet applique la programmation linÃ©aire Ã  un problÃ¨me classique dâ€™allocation de ressources : la maximisation du profit dans la production de canapÃ©s. Sous contraintes de temps, de matÃ©riaux et de capacitÃ© de marchÃ©, lâ€™analyse dÃ©termine la combinaison optimale de produits, accompagnÃ©e dâ€™une analyse de sensibilitÃ© et dâ€™une visualisation de la rÃ©gion rÃ©alisable.\n    \n    \n      Python\n      NumPy\n      SciPy\n      Programmation linÃ©aire\n      Optimisation\n      Matplotlib\n    \n    Voir le projet â†’\n  \n\n\n  \n\n  \n    Contact\n    Pour Ã©changer sur mes travaux ou mon parcours\n    \n      ğŸ“§ Me contacter"
  },
  {
    "objectID": "skills-fixed.html",
    "href": "skills-fixed.html",
    "title": "CompÃ©tences",
    "section": "",
    "text": "ğŸ¯ Mes Domaines d'Expertise\n    Cliquez sur une carte pour explorer les compÃ©tences en dÃ©tail. Survolez les catÃ©gories pour voir le contenu.\n  \n\n  \n    \n    \n    \n      \n      \n      \n        \n          â†\n          ğŸ“š\n          \n            CONNAISSANCES THÃ‰ORIQUES\n            Ã‰conomie, MathÃ©matiques, Statistiques & Ã‰conomÃ©trie\n          \n        \n        \n          \n            \n              \n                â–¶\n                ğŸŒ\n                Langues\n                3\n              \n              \n                ğŸ‡«ğŸ‡· FranÃ§ais (Natif)\n                ğŸ‡¬ğŸ‡§ Anglais (C1/C2)\n                ğŸ‡ªğŸ‡¸ Espagnol (IntermÃ©diaire)\n              \n            \n            \n              \n                â–¶\n                ğŸ“Š\n                Ã‰conomie\n                5\n              \n              \n                MicroÃ©conomie\n                MacroÃ©conomie\n                Ã‰conomie industrielle\n                ThÃ©orie des jeux\n                Ã‰conomie publique\n              \n            \n            \n              \n                â–¶\n                ğŸ”¢\n                MathÃ©matiques\n                4\n              \n              \n                AlgÃ¨bre linÃ©aire\n                Analyse\n                Optimisation\n                ProbabilitÃ©s\n              \n            \n            \n              \n                â–¶\n                ğŸ“ˆ\n                Statistiques\n                5\n              \n              \n                Tests d'hypothÃ¨ses\n                RÃ©gression\n                SÃ©ries temporelles\n                InfÃ©rence bayÃ©sienne\n                Analyse multivariÃ©e\n              \n            \n            \n              \n                â–¶\n                ğŸ“‰\n                Ã‰conomÃ©trie\n                5\n              \n              \n                Variables instrumentales\n                Diff-in-Diff\n                ARIMA / GARCH\n                Panel Data\n                RDD\n              \n            \n          \n        \n      \n    \n\n    \n    \n      \n      \n      \n        \n          â†\n          ğŸ¤–\n          \n            MACHINE LEARNING & IA\n            ModÃ¨les prÃ©dictifs, Deep Learning & Frameworks\n          \n        \n        \n          \n            \n              \n                â–¶\n                ğŸ¯\n                Apprentissage SupervisÃ©\n                5\n              \n              \n                RÃ©gression linÃ©aire/logistique\n                Random Forest\n                XGBoost / LightGBM\n                SVM\n                K-NN\n              \n            \n            \n              \n                â–¶\n                ğŸ”\n                Apprentissage Non SupervisÃ©\n                4\n              \n              \n                K-Means\n                DBSCAN\n                PCA / t-SNE\n                Clustering hiÃ©rarchique\n              \n            \n            \n              \n                â–¶\n                ğŸ§ \n                Deep Learning\n                4\n              \n              \n                RÃ©seaux de neurones\n                CNN (Computer Vision)\n                RNN / LSTM\n                Transformers\n              \n            \n            \n              \n                â–¶\n                âš™ï¸\n                Frameworks & Outils\n                5\n              \n              \n                Scikit-learn\n                TensorFlow / Keras\n                PyTorch\n                Tidymodels (R)\n                H2O.ai\n              \n            \n          \n        \n      \n    \n\n    \n    \n      \n      \n      \n        \n          â†\n          ğŸ’»\n          \n            TECHNOLOGIES & OUTILS\n            Programmation, DataViz, Cloud & DevOps\n          \n        \n        \n          \n            \n              \n                â–¶\n                ğŸ\n                Langages de Programmation\n                4\n              \n              \n                Python (Expert)\n                R (AvancÃ©)\n                SQL (AvancÃ©)\n                Bash / Shell\n              \n            \n            \n              \n                â–¶\n                ğŸ“Š\n                Data Visualization\n                5\n              \n              \n                Power BI\n                Tableau\n                Plotly / Dash\n                ggplot2 (R)\n                Matplotlib / Seaborn\n              \n            \n            \n              \n                â–¶\n                â˜ï¸\n                Cloud & DevOps\n                6\n              \n              \n                AWS (S3, EC2, Redshift)\n                GCP (BigQuery)\n                Git / GitHub\n                Docker\n                Kubernetes\n                CI/CD Pipelines\n              \n            \n          \n        \n      \n    \n\n    \n    \n      \n      \n      \n        \n          â†\n          ğŸ—£ï¸\n          \n            SOFT SKILLS & LANGUES\n            CompÃ©tences transversales & communication\n          \n        \n        \n          \n            \n              \n                â–¶\n                ğŸ’¡\n                Aptitudes Professionnelles\n                6\n              \n              \n                Analyse Critique\n                Communication\n                Rigueur Scientifique\n                Leadership\n                Problem Solving\n                Gestion de Projet\n              \n            \n            \n              \n                â–¶\n                ğŸŒ\n                Langues\n                3\n              \n              \n                ğŸ‡«ğŸ‡· FranÃ§ais (Natif)\n                ğŸ‡¬ğŸ‡§ Anglais (C1/C2)\n                ğŸ‡ªğŸ‡¸ Espagnol (IntermÃ©diaire)\n              \n            \n          \n        \n      \n    \n\n  \n\n  \n    ğŸ  Accueil\n    /"
  }
]